import json
import logging
import re
from typing import List, Dict, Set, Tuple

from .git_db import GitDB
from .sqlite_db import DatabaseManager
from .git_object_storage import GitObjectHistoryReader  # Reuse parsing logic

logger = logging.getLogger(__name__)


class Hydrator:
    """
    è´Ÿè´£å°† Git å¯¹è±¡å†å²è®°å½•åŒæ­¥ï¼ˆè¡¥æ°´ï¼‰åˆ° SQLite æ•°æ®åº“ã€‚
    """

    def __init__(self, git_db: GitDB, db_manager: DatabaseManager):
        self.git_db = git_db
        self.db_manager = db_manager
        # å¤ç”¨ Reader ä¸­çš„äºŒè¿›åˆ¶è§£æé€»è¾‘ï¼Œé¿å…ä»£ç é‡å¤
        self._parser = GitObjectHistoryReader(git_db)

    def _get_missing_commit_hashes(self) -> Set[str]:
        """
        è®¡ç®—å­˜åœ¨äº Git ä¸­ä½†ç¼ºå¤±äº SQLite çš„ commit å“ˆå¸Œé›†åˆã€‚
        """
        logger.debug("æ­£åœ¨è®¡ç®—éœ€è¦è¡¥æ°´çš„ Commit...")
        all_git_heads = self.git_db.get_all_ref_heads("refs/quipu/")
        if not all_git_heads:
            return set()

        git_log_entries = self.git_db.log_ref(all_git_heads)
        git_hashes = {entry["hash"] for entry in git_log_entries}
        
        db_hashes = self.db_manager.get_all_node_hashes()
        
        missing_hashes = git_hashes - db_hashes
        logger.info(f"å‘ç° {len(missing_hashes)} ä¸ªéœ€è¦è¡¥æ°´çš„èŠ‚ç‚¹ã€‚")
        return missing_hashes

    def sync(self):
        """
        æ‰§è¡Œå¢é‡è¡¥æ°´æ“ä½œã€‚
        """
        missing_hashes = self._get_missing_commit_hashes()
        if not missing_hashes:
            logger.debug("âœ… æ•°æ®åº“ä¸ Git å†å²ä¸€è‡´ï¼Œæ— éœ€è¡¥æ°´ã€‚")
            return

        all_git_logs = self.git_db.log_ref(self.git_db.get_all_ref_heads("refs/quipu/"))
        log_map = {entry["hash"]: entry for entry in all_git_logs}

        # --- æ‰¹é‡å‡†å¤‡æ•°æ® ---
        nodes_to_insert: List[Tuple] = []
        edges_to_insert: List[Tuple] = []

        # 1. æ‰¹é‡è·å– Trees
        tree_hashes = [log_map[h]["tree"] for h in missing_hashes]
        trees_content = self.git_db.batch_cat_file(tree_hashes)

        # 2. è§£æ Trees, æ‰¹é‡è·å– Metas
        tree_to_meta_blob: Dict[str, str] = {}
        meta_blob_hashes: List[str] = []
        for tree_hash, content_bytes in trees_content.items():
            entries = self._parser._parse_tree_binary(content_bytes)
            if "metadata.json" in entries:
                blob_hash = entries["metadata.json"]
                tree_to_meta_blob[tree_hash] = blob_hash
                meta_blob_hashes.append(blob_hash)

        metas_content = self.git_db.batch_cat_file(meta_blob_hashes)

        # 3. æ„å»ºæ’å…¥æ•°æ®
        for commit_hash in missing_hashes:
            log_entry = log_map[commit_hash]
            tree_hash = log_entry["tree"]
            
            meta_blob_hash = tree_to_meta_blob.get(tree_hash)
            if not meta_blob_hash:
                logger.warning(f"è·³è¿‡ {commit_hash[:7]}: æ‰¾ä¸åˆ° metadata.json")
                continue

            meta_bytes = metas_content.get(meta_blob_hash)
            if not meta_bytes:
                logger.warning(f"è·³è¿‡ {commit_hash[:7]}: æ‰¾ä¸åˆ° metadata blob")
                continue
            
            output_tree = self._parser._parse_output_tree_from_body(log_entry["body"])
            if not output_tree:
                logger.warning(f"è·³è¿‡ {commit_hash[:7]}: æ‰¾ä¸åˆ° Output-Tree trailer")
                continue

            try:
                meta_data = json.loads(meta_bytes)
                nodes_to_insert.append((
                    commit_hash,
                    output_tree,
                    meta_data.get("type", "unknown"),
                    float(meta_data.get("exec", {}).get("start") or log_entry["timestamp"]),
                    meta_data.get("summary", "No summary"),
                    meta_data.get("generator", {}).get("id"),
                    meta_bytes.decode('utf-8'),
                    None  # plan_md_cache is NULL for cold data
                ))

                # å¤„ç†è¾¹å…³ç³»
                parent_hashes = log_entry["parent"].split()
                for p_hash in parent_hashes:
                    edges_to_insert.append((commit_hash, p_hash))
            except (json.JSONDecodeError, KeyError) as e:
                logger.error(f"è§£æ {commit_hash[:7]} çš„å…ƒæ•°æ®å¤±è´¥: {e}")

        # --- æ‰¹é‡å†™å…¥æ•°æ®åº“ ---
        if nodes_to_insert:
            self.db_manager.batch_insert_nodes(nodes_to_insert)
            logger.info(f"ğŸ’§ {len(nodes_to_insert)} ä¸ªèŠ‚ç‚¹å…ƒæ•°æ®å·²è¡¥æ°´ã€‚")
        if edges_to_insert:
            self.db_manager.batch_insert_edges(edges_to_insert)
            logger.info(f"ğŸ’§ {len(edges_to_insert)} æ¡è¾¹å…³ç³»å·²è¡¥æ°´ã€‚")
