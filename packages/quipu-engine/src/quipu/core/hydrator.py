import json
import logging
import re
from typing import List, Dict, Set, Tuple, Optional

from .git_db import GitDB
from .sqlite_db import DatabaseManager
from .git_object_storage import GitObjectHistoryReader  # Reuse parsing logic

logger = logging.getLogger(__name__)


class Hydrator:
    """
    è´Ÿè´£å°† Git å¯¹è±¡å†å²è®°å½•åŒæ­¥ï¼ˆè¡¥æ°´ï¼‰åˆ° SQLite æ•°æ®åº“ã€‚
    """

    def __init__(self, git_db: GitDB, db_manager: DatabaseManager):
        self.git_db = git_db
        self.db_manager = db_manager
        # å¤ç”¨ Reader ä¸­çš„äºŒè¿›åˆ¶è§£æé€»è¾‘ï¼Œé¿å…ä»£ç é‡å¤
        self._parser = GitObjectHistoryReader(git_db)

    def _get_owner_from_ref(self, ref_name: str, local_user_id: str) -> Optional[str]:
        """ä» Git ref è·¯å¾„ä¸­è§£æ owner_idã€‚"""
        # åŒ¹é… remote é•œåƒ: refs/quipu/remotes/<remote_name>/<user_id>/heads/...
        remote_match = re.match(r"refs/quipu/remotes/[^/]+/([^/]+)/heads/.*", ref_name)
        if remote_match:
            return remote_match.group(1)

        # åŒ¹é… local heads
        if ref_name.startswith("refs/quipu/local/heads/"):
            return local_user_id

        return None

    def _get_missing_commits_with_owner(self, local_user_id: str) -> Dict[str, str]:
        """
        è®¡ç®— Git ä¸­å­˜åœ¨ä½† SQLite ç¼ºå¤±çš„ commitï¼Œå¹¶ç¡®å®šå…¶æ‰€æœ‰è€…ã€‚
        è¿”å› {commit_hash: owner_id} å­—å…¸ã€‚
        """
        logger.debug("æ­£åœ¨è®¡ç®—éœ€è¦è¡¥æ°´çš„ Commit åŠå…¶æ‰€æœ‰è€…...")
        ref_tuples = self.git_db.get_all_ref_heads("refs/quipu/")
        if not ref_tuples:
            return {}

        commit_to_owner: Dict[str, str] = {}
        for commit_hash, ref_name in ref_tuples:
            # ä¸€ä¸ª commit å¯èƒ½è¢«å¤šä¸ª ref æŒ‡å‘ (e.g., local å’Œ remote mirror)
            # åªè¦èƒ½ç¡®å®šä¸€ä¸ªæ‰€æœ‰è€…å³å¯ã€‚
            if commit_hash in commit_to_owner:
                continue
            
            owner_id = self._get_owner_from_ref(ref_name, local_user_id)
            if owner_id:
                commit_to_owner[commit_hash] = owner_id

        if not commit_to_owner:
            return {}

        db_hashes = self.db_manager.get_all_node_hashes()
        
        missing_commits = {
            commit: owner for commit, owner in commit_to_owner.items() if commit not in db_hashes
        }
        
        logger.info(f"å‘ç° {len(missing_commits)} ä¸ªéœ€è¦è¡¥æ°´çš„èŠ‚ç‚¹ã€‚")
        return missing_commits

    def sync(self, local_user_id: str):
        """
        æ‰§è¡Œå¢é‡è¡¥æ°´æ“ä½œã€‚
        """
        missing_commits = self._get_missing_commits_with_owner(local_user_id)
        if not missing_commits:
            logger.debug("âœ… æ•°æ®åº“ä¸ Git å†å²ä¸€è‡´ï¼Œæ— éœ€è¡¥æ°´ã€‚")
            return

        missing_hashes = list(missing_commits.keys())
        all_git_logs = self.git_db.log_ref(missing_hashes) # Log only missing commits for efficiency
        log_map = {entry["hash"]: entry for entry in all_git_logs}

        # --- æ‰¹é‡å‡†å¤‡æ•°æ® ---
        nodes_to_insert: List[Tuple] = []
        edges_to_insert: List[Tuple] = []

        # 1. æ‰¹é‡è·å– Trees
        tree_hashes = [log_map[h]["tree"] for h in missing_hashes if h in log_map]
        trees_content = self.git_db.batch_cat_file(tree_hashes)

        # 2. è§£æ Trees, æ‰¹é‡è·å– Metas
        tree_to_meta_blob: Dict[str, str] = {}
        meta_blob_hashes: List[str] = []
        for tree_hash, content_bytes in trees_content.items():
            entries = self._parser._parse_tree_binary(content_bytes)
            if "metadata.json" in entries:
                blob_hash = entries["metadata.json"]
                tree_to_meta_blob[tree_hash] = blob_hash
                meta_blob_hashes.append(blob_hash)

        metas_content = self.git_db.batch_cat_file(meta_blob_hashes)

        # 3. æ„å»ºæ’å…¥æ•°æ®
        for commit_hash in missing_hashes:
            if commit_hash not in log_map: continue
            
            log_entry = log_map[commit_hash]
            tree_hash = log_entry["tree"]
            owner_id = missing_commits[commit_hash]

            meta_blob_hash = tree_to_meta_blob.get(tree_hash)
            if not meta_blob_hash:
                logger.warning(f"è·³è¿‡ {commit_hash[:7]}: æ‰¾ä¸åˆ° metadata.json")
                continue

            meta_bytes = metas_content.get(meta_blob_hash)
            if not meta_bytes:
                logger.warning(f"è·³è¿‡ {commit_hash[:7]}: æ‰¾ä¸åˆ° metadata blob")
                continue

            output_tree = self._parser._parse_output_tree_from_body(log_entry["body"])
            if not output_tree:
                logger.warning(f"è·³è¿‡ {commit_hash[:7]}: æ‰¾ä¸åˆ° Output-Tree trailer")
                continue

            try:
                meta_data = json.loads(meta_bytes)
                nodes_to_insert.append(
                    (
                        commit_hash,
                        owner_id,
                        output_tree,
                        meta_data.get("type", "unknown"),
                        float(meta_data.get("exec", {}).get("start") or log_entry["timestamp"]),
                        meta_data.get("summary", "No summary"),
                        meta_data.get("generator", {}).get("id"),
                        meta_bytes.decode("utf-8"),
                        None,  # plan_md_cache is NULL for cold data
                    )
                )

                # å¤„ç†è¾¹å…³ç³»
                parent_hashes = log_entry["parent"].split()
                for p_hash in parent_hashes:
                    edges_to_insert.append((commit_hash, p_hash))
            except (json.JSONDecodeError, KeyError) as e:
                logger.error(f"è§£æ {commit_hash[:7]} çš„å…ƒæ•°æ®å¤±è´¥: {e}")

        # --- æ‰¹é‡å†™å…¥æ•°æ®åº“ ---
        if nodes_to_insert:
            self.db_manager.batch_insert_nodes(nodes_to_insert)
            logger.info(f"ğŸ’§ {len(nodes_to_insert)} ä¸ªèŠ‚ç‚¹å…ƒæ•°æ®å·²è¡¥æ°´ã€‚")
        if edges_to_insert:
            self.db_manager.batch_insert_edges(edges_to_insert)
            logger.info(f"ğŸ’§ {len(edges_to_insert)} æ¡è¾¹å…³ç³»å·²è¡¥æ°´ã€‚")